{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline definition and running\n",
    "\n",
    "This notebook serves as the core of the assignment solution for the Chicago Taxi Trips task. It defines all the necessary steps and runs the pipeline on Vertex Pipelines. The other notebooks will refer back to parts of this notebook or use the results of a pipeline run created here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "Xd-iP9wEaENu",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.8.1\n",
      "TFX version: 1.8.0\n",
      "KFP version: 1.8.12\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print('TensorFlow version: {}'.format(tf.__version__))\n",
    "from tfx import v1 as tfx_v1\n",
    "import tfx\n",
    "print('TFX version: {}'.format(tfx.__version__))\n",
    "import kfp\n",
    "print('KFP version: {}'.format(kfp.__version__))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aDtLdSkvqPHe"
   },
   "source": [
    "### Setting up variables\n",
    "\n",
    "This section sets up the GCP variables used for the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {
    "id": "EcUseqJaE2XN",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PIPELINE_ROOT: gs://mormota/pipeline_root/taxi-vertex-pipelines\n"
     ]
    }
   ],
   "source": [
    "GOOGLE_CLOUD_PROJECT = 'aliz-ml-spec-2022-dev'\n",
    "GOOGLE_CLOUD_REGION = 'us-central1'\n",
    "GCS_BUCKET_NAME = 'mormota'\n",
    "\n",
    "PIPELINE_NAME = 'taxi-vertex-pipelines'\n",
    "\n",
    "# Path to various pipeline artifact.\n",
    "PIPELINE_ROOT = 'gs://{}/pipeline_root/{}'.format(\n",
    "    GCS_BUCKET_NAME, PIPELINE_NAME)\n",
    "\n",
    "# Paths for users' Python module.\n",
    "MODULE_ROOT = 'gs://{}/pipeline_module/{}'.format(\n",
    "    GCS_BUCKET_NAME, PIPELINE_NAME)\n",
    "\n",
    "# Paths for input data.\n",
    "DATA_ROOT = 'gs://{}/data/{}'.format(GCS_BUCKET_NAME, PIPELINE_NAME)\n",
    "\n",
    "# This is the path where your model will be pushed for serving.\n",
    "SERVING_MODEL_DIR = 'gs://{}/serving_model/{}'.format(\n",
    "    GCS_BUCKET_NAME, PIPELINE_NAME)\n",
    "\n",
    "ENDPOINT_NAME = 'prediction-' + PIPELINE_NAME\n",
    "\n",
    "print('PIPELINE_ROOT: {}'.format(PIPELINE_ROOT))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VkWdxe4TXRHk",
    "tags": []
   },
   "outputs": [],
   "source": [
    "!gcloud config set project {GOOGLE_CLOUD_PROJECT}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8F2SRwRLSYGa"
   },
   "source": [
    "### Prepare data\n",
    "\n",
    "To simplify the process, we are going to use the CsvExampleGen component. To do so, we export the required portion of the dataset to a GCS bucket using the query below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%bigquery\n",
    "\n",
    "EXPORT DATA\n",
    "  OPTIONS (\n",
    "    uri = 'gs://mormota/data/taxi_data/taxi_*.csv',\n",
    "    format = 'CSV',\n",
    "    overwrite = true,\n",
    "    header = true,\n",
    "    field_delimiter = ',')\n",
    "AS (\n",
    "  SELECT\n",
    "    trip_id,\n",
    "    CAST(TripStartYear AS STRING) AS TripStartYear,\n",
    "     CAST(TripStartMonth AS STRING) AS TripStartMonth,\n",
    "      CAST(TripStartDay AS STRING) AS TripStartDay,\n",
    "       CAST(TripStartHour AS STRING) AS TripStartHour,\n",
    "        CAST(TripStartMinute AS STRING) AS TripStartMinute,\n",
    "         CAST(pickup_census_tract AS STRING) AS pickup_census_tract,\n",
    "          CAST(dropoff_census_tract AS STRING) AS dropoff_census_tract,\n",
    "\n",
    "    IFNULL(fare, 0) AS fare,\n",
    "     IFNULL(historical_tripDuration, 0) AS historical_tripDuration,\n",
    "      IFNULL(histOneWeek_tripDuration, 0) AS histOneWeek_tripDuration,\n",
    "       IFNULL(historical_tripDistance, 0) AS historical_tripDistance,\n",
    "        IFNULL(histOneWeek_tripDistance, 0) AS histOneWeek_tripDistance,\n",
    "         IFNULL(rawDistance, 0) AS rawDistance, \n",
    "    CASE\n",
    "        WHEN trip_start_timestamp >= '2021-01-01 00:00:00 UTC' THEN 'test'\n",
    "        WHEN trip_start_timestamp < '2021-01-01 00:00:00 UTC' AND ABS(MOD(FARM_FINGERPRINT(trip_id), 20)) = 0 THEN 'eval'\n",
    "        ELSE 'train'\n",
    "    END AS selector\n",
    "FROM `aliz-ml-spec-2022-dev.mormota.Demo1_MLdataset`\n",
    "WHERE trip_start_timestamp >= '2020-01-01 00:00:00 UTC'\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nH6gizcpSwWV"
   },
   "source": [
    "### Pipeline modules\n",
    "\n",
    "The following blocks create the preprocessing and model module code and upload the files to GCS. This last step is necessary for TFX to be able to use our custom code in the different components.\n",
    "\n",
    "#### Preprocessing\n",
    "\n",
    "The preprocessing transforms the categorical features into vocabularies and standardises the numerical features.\n",
    "\n",
    "#### Model\n",
    "\n",
    "The model is a simple DNN that is made up of two blocks. The first processes the inputs and embeds taxi trips in an implicit feature space. The second is a dense block that leads to the final predictions.\n",
    "\n",
    "#### Hyperparameter tuning\n",
    "\n",
    "The hyperparameter tuning section uses Keras tuning to perform a hyperparameter search on the previously defined model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess_module_file = 'preprocess.py'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting preprocess.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {preprocess_module_file}\n",
    "\n",
    "import tensorflow_transform as transform\n",
    "import tensorflow as tf\n",
    "\n",
    "_VOCAB_FEATURE_KEYS = [\n",
    "    'TripStartYear',\n",
    "    'TripStartMonth',\n",
    "    'TripStartDay',\n",
    "    'TripStartHour',\n",
    "    'TripStartMinute',\n",
    "    'pickup_census_tract',\n",
    "    'dropoff_census_tract'\n",
    "]\n",
    "\n",
    "_VOCAB_SIZE = 10\n",
    "_OOV_SIZE = 5\n",
    "\n",
    "_FARE_KEY = 'fare'\n",
    "\n",
    "_LABEL_KEY = 'fare'\n",
    "\n",
    "_DENSE_FLOAT_FEATURE_KEYS = [\n",
    "    'fare',\n",
    " 'historical_tripDuration',\n",
    " 'histOneWeek_tripDuration',\n",
    " 'historical_tripDistance',\n",
    " 'histOneWeek_tripDistance',\n",
    " 'rawDistance',\n",
    "]\n",
    "\n",
    "_BUCKET_FEATURE_KEYS = []\n",
    "\n",
    "_FEATURE_BUCKET_COUNT = 10\n",
    "\n",
    "_CATEGORICAL_FEATURE_KEYS = []\n",
    "\n",
    "def _transformed_name(key):\n",
    "  return key# + '_xf'\n",
    "\n",
    "def preprocessing_fn(inputs):\n",
    "  \"\"\"tf.transform's callback function for preprocessing inputs.\n",
    "\n",
    "  Args:\n",
    "    inputs: map from feature keys to raw not-yet-transformed features.\n",
    "\n",
    "  Returns:\n",
    "    Map from string feature key to transformed feature operations.\n",
    "  \"\"\"\n",
    "  outputs = {}\n",
    "  for key in _DENSE_FLOAT_FEATURE_KEYS:\n",
    "    outputs[_transformed_name(key)] = transform.scale_to_z_score(inputs[key])\n",
    "\n",
    "  for key in _VOCAB_FEATURE_KEYS:\n",
    "    outputs[_transformed_name(\n",
    "        key)] = transform.compute_and_apply_vocabulary(\n",
    "            inputs[key],\n",
    "            top_k=_VOCAB_SIZE,\n",
    "            num_oov_buckets=_OOV_SIZE)\n",
    "\n",
    "  for key in _BUCKET_FEATURE_KEYS:\n",
    "    outputs[_transformed_name(key)] = transform.bucketize(\n",
    "        inputs[key], _FEATURE_BUCKET_COUNT)\n",
    "\n",
    "  for key in _CATEGORICAL_FEATURE_KEYS:\n",
    "    outputs[_transformed_name(key)] = inputs[key]\n",
    "\n",
    "  taxi_fare = inputs[_FARE_KEY]\n",
    "  tips = inputs[_LABEL_KEY]\n",
    "  outputs[_transformed_name(_LABEL_KEY)] = tf.where(\n",
    "      tf.is_nan(taxi_fare),\n",
    "      tf.cast(tf.zeros_like(taxi_fare), tf.int64),\n",
    "      # Test if the tip was > 20% of the fare.\n",
    "      tf.cast(\n",
    "          tf.greater(tips, tf.multiply(taxi_fare, tf.constant(0.2))), tf.int64))\n",
    "\n",
    "  return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {
    "id": "aES7Hv5QTDK3",
    "tags": []
   },
   "outputs": [],
   "source": [
    "_trainer_module_file = 'taxi_trainer.py'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {
    "id": "Gnc67uQNTDfW",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting taxi_trainer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {_trainer_module_file}\n",
    "\n",
    "from typing import List\n",
    "from absl import logging\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow_transform.tf_metadata import schema_utils\n",
    "import tensorflow_data_validation as tfdv\n",
    "from tfx import v1 as tfx_v1\n",
    "from tfx_bsl.public import tfxio\n",
    "from tensorflow_metadata.proto.v0 import schema_pb2\n",
    "\n",
    "_FEATURE_KEYS = [#'trip_id',\n",
    " 'TripStartYear',\n",
    " 'TripStartMonth',\n",
    " 'TripStartDay',\n",
    " 'TripStartHour',\n",
    " 'TripStartMinute',\n",
    " 'pickup_census_tract',\n",
    " 'dropoff_census_tract',\n",
    " #'fare',\n",
    " 'historical_tripDuration',\n",
    " 'histOneWeek_tripDuration',\n",
    " 'historical_tripDistance',\n",
    " 'histOneWeek_tripDistance',\n",
    " 'rawDistance',\n",
    " 'selector']\n",
    "_LABEL_KEY = 'fare'\n",
    "\n",
    "_TRAIN_BATCH_SIZE = 20\n",
    "_EVAL_BATCH_SIZE = 10\n",
    "\n",
    "\n",
    "def _get_hyperparameters() -> keras_tuner.HyperParameters:\n",
    "  \"\"\"Returns hyperparameters for building Keras model.\"\"\"\n",
    "  hp = keras_tuner.HyperParameters()\n",
    "    \n",
    "  hp.Choice('learning_rate', [1e-2, 1e-3], default=1e-2)\n",
    "  hp.Int('num_layers', 1, 3, default=2)\n",
    "  return hp\n",
    "\n",
    "\n",
    "def _input_fn(file_pattern: List[str],\n",
    "              data_accessor: tfx_v1.components.DataAccessor,\n",
    "              schema: schema_pb2.Schema,\n",
    "              batch_size: int) -> tf.data.Dataset:\n",
    "  \"\"\"Generates features and label for training.\n",
    "\n",
    "  Args:\n",
    "    file_pattern: List of paths or patterns of input tfrecord files.\n",
    "    data_accessor: DataAccessor for converting input to RecordBatch.\n",
    "    schema: schema of the input data.\n",
    "    batch_size: representing the number of consecutive elements of returned\n",
    "      dataset to combine in a single batch\n",
    "\n",
    "  Returns:\n",
    "    A dataset that contains (features, indices) tuple where features is a\n",
    "      dictionary of Tensors, and indices is a single Tensor of label indices.\n",
    "  \"\"\"\n",
    "  return data_accessor.tf_dataset_factory(\n",
    "      file_pattern,\n",
    "      tfxio.TensorFlowDatasetOptions(\n",
    "          batch_size=batch_size, label_key=_LABEL_KEY),\n",
    "      schema=schema).repeat()\n",
    "\n",
    "\n",
    "def _make_keras_model(hparams: keras_tuner.HyperParameters) -> tf.keras.Model:\n",
    "  \"\"\"Creates a DNN Keras model for predicting taxi trips fare for the Chicago Taxi Trips assignment.\n",
    "\n",
    "  Returns:\n",
    "    A Keras Model.\n",
    "  \"\"\"\n",
    "  # The model below is built with Functional API, please refer to\n",
    "  # https://www.tensorflow.org/guide/keras/overview for all API options.\n",
    "  inputs = [keras.layers.Input(shape=(1,), name=f) for f in _FEATURE_KEYS]\n",
    "  d = keras.layers.concatenate(inputs)\n",
    "  for _ in range(int(hparams.get('num_layers'))):\n",
    "    d = keras.layers.Dense(8, activation='relu')(d)\n",
    "  outputs = keras.layers.Dense(3)(d)\n",
    "\n",
    "  model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "  model.compile(\n",
    "      optimizer=keras.optimizers.Adam(hparams.get('learning_rate')),\n",
    "      loss=tf.keras.losses.MeanSquaredError(),\n",
    "      metrics=[keras.metrics.MeanSquaredError()])\n",
    "\n",
    "  model.summary(print_fn=logging.info)\n",
    "  return model\n",
    "\n",
    "def tuner_fn(fn_args: tfx.components.FnArgs) -> tfx.components.TunerFnResult:\n",
    "  \"\"\"Build the tuner using the KerasTuner API.\n",
    "  Args:\n",
    "    fn_args: Holds args as name/value pairs.\n",
    "      - working_dir: working dir for tuning.\n",
    "      - train_files: List of file paths containing training tf.Example data.\n",
    "      - eval_files: List of file paths containing eval tf.Example data.\n",
    "      - train_steps: number of train steps.\n",
    "      - eval_steps: number of eval steps.\n",
    "      - schema_path: optional schema of the input data.\n",
    "      - transform_graph_path: optional transform graph produced by TFT.\n",
    "  Returns:\n",
    "    A namedtuple contains the following:\n",
    "      - tuner: A BaseTuner that will be used for tuning.\n",
    "      - fit_kwargs: Args to pass to tuner's run_trial function for fitting the\n",
    "                    model , e.g., the training and validation dataset. Required\n",
    "                    args depend on the above tuner's implementation.\n",
    "  \"\"\"\n",
    "  tuner = keras_tuner.RandomSearch(\n",
    "      _make_keras_model,\n",
    "      max_trials=6,\n",
    "      hyperparameters=_get_hyperparameters(),\n",
    "      allow_new_entries=False,\n",
    "      objective=keras_tuner.Objective('val_root_mean_squared_error', 'max'),\n",
    "      directory=fn_args.working_dir,\n",
    "      project_name='taxi_tuning')\n",
    "\n",
    "  transform_graph = tft.TFTransformOutput(fn_args.transform_graph_path)\n",
    "\n",
    "  train_dataset = base.input_fn(\n",
    "      fn_args.train_files,\n",
    "      fn_args.data_accessor,\n",
    "      transform_graph,\n",
    "      base.TRAIN_BATCH_SIZE)\n",
    "\n",
    "  eval_dataset = base.input_fn(\n",
    "      fn_args.eval_files,\n",
    "      fn_args.data_accessor,\n",
    "      transform_graph,\n",
    "      base.EVAL_BATCH_SIZE)\n",
    "\n",
    "  return tfx.components.TunerFnResult(\n",
    "      tuner=tuner,\n",
    "      fit_kwargs={\n",
    "          'x': train_dataset,\n",
    "          'validation_data': eval_dataset,\n",
    "          'steps_per_epoch': fn_args.train_steps,\n",
    "          'validation_steps': fn_args.eval_steps\n",
    "      })\n",
    "\n",
    "\n",
    "def run_fn(fn_args: tfx_v1.components.FnArgs):\n",
    "  \"\"\"Train the model based on given args.\n",
    "\n",
    "  Args:\n",
    "    fn_args: Holds args used to train the model as name/value pairs.\n",
    "  \"\"\"\n",
    "\n",
    "  schema = tfdv.load_schema_text(fn_args.schema_path)\n",
    "  #schema = schema_utils.schema_from_feature_spec(_FEATURE_SPEC)\n",
    "  train_dataset = _input_fn(\n",
    "      fn_args.train_files,\n",
    "      fn_args.data_accessor,\n",
    "      schema,\n",
    "      batch_size=_TRAIN_BATCH_SIZE)\n",
    "  eval_dataset = _input_fn(\n",
    "      fn_args.eval_files,\n",
    "      fn_args.data_accessor,\n",
    "      schema,\n",
    "      batch_size=_EVAL_BATCH_SIZE)\n",
    "\n",
    "  model = _make_keras_model()\n",
    "  model.fit(\n",
    "      train_dataset,\n",
    "      steps_per_epoch=fn_args.train_steps,\n",
    "      validation_data=eval_dataset,\n",
    "      validation_steps=fn_args.eval_steps)\n",
    "\n",
    "  model.save(fn_args.serving_model_dir, save_format='tf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-LsYx8MpYvPv"
   },
   "source": [
    "Copy the module file to GCS which can be accessed from the pipeline components.\n",
    "Because model training happens on GCP, we need to upload this model definition. \n",
    "\n",
    "Otherwise, you might want to build a container image including the module file\n",
    "and use the image to run the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {
    "id": "rMMs5wuNYAbc",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying file://taxi_trainer.py [Content-Type=text/x-python]...\n",
      "/ [1 files][  5.5 KiB/  5.5 KiB]                                                \n",
      "Operation completed over 1 objects/5.5 KiB.                                      \n"
     ]
    }
   ],
   "source": [
    "!gsutil cp {_trainer_module_file} {MODULE_ROOT}/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying file://preprocess.py [Content-Type=text/x-python]...\n",
      "/ [1 files][  1.8 KiB/  1.8 KiB]                                                \n",
      "Operation completed over 1 objects/1.8 KiB.                                      \n"
     ]
    }
   ],
   "source": [
    "!gsutil cp {preprocess_module_file} {MODULE_ROOT}/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w3OkNz3gTLwM"
   },
   "source": [
    "### Pipeline definition\n",
    "\n",
    "The following block defines our TFX pipeline. It is made up of the following components:\n",
    "\n",
    "- CsvExampleGen: Loads our dataset from Google Cloud Storage\n",
    "- SchemaGen: Creates a TF schema from our dataset to be used in later components\n",
    "- ExampleValidator: Allows us to check for data skew later on\n",
    "- Transform: Performs preprocessing on the data\n",
    "- Tuner: Performs hyperparameter tuning\n",
    "- Trainer: Trains the model\n",
    "- Pusher: Deploys the model to the endpoint on Vertex AI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {
    "id": "M49yYVNBTPd4",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def _create_pipeline(pipeline_name: str, pipeline_root: str, data_root: str,\n",
    "                     module_file: str, serving_model_dir: str, project_id: str,\n",
    "                     endpoint_name: str, region: str,\n",
    "                     ) -> tfx_v1.dsl.Pipeline:\n",
    "  \"\"\"Defines a pipeline for the Chicago Taxi Trips assignment\"\"\"\n",
    "\n",
    "  example_gen = tfx_v1.components.CsvExampleGen(input_base='gs://mormota/data/taxi_data')\n",
    "  compute_eval_stats = tfx_v1.components.StatisticsGen(\n",
    "      examples=example_gen.outputs['examples'],\n",
    "      )\n",
    "  schema_gen = tfx_v1.components.SchemaGen(\n",
    "    statistics=compute_eval_stats.outputs['statistics'])\n",
    "    \n",
    "  validate_stats = tfx_v1.components.ExampleValidator(\n",
    "      statistics=compute_eval_stats.outputs['statistics'],\n",
    "      schema=schema_gen.outputs['schema']\n",
    "      )\n",
    "\n",
    "  transform = tfx_v1.components.Transform(\n",
    "    examples=example_gen.outputs['examples'],\n",
    "    schema=schema_gen.outputs['schema'],\n",
    "    module_file=preprocess_module_file)\n",
    "    \n",
    "  vertex_job_spec = {\n",
    "      'project': project_id,\n",
    "      'worker_pool_specs': [{\n",
    "          'machine_spec': {\n",
    "              'machine_type': 'n1-standard-4',\n",
    "          },\n",
    "          'replica_count': 1,\n",
    "          'container_spec': {\n",
    "              'image_uri': 'gcr.io/tfx-oss-public/tfx:{}'.format(tfx.__version__),\n",
    "          },\n",
    "      }],\n",
    "  }\n",
    "\n",
    "\n",
    "  trainer = tfx_v1.components.Trainer(\n",
    "      module_file=module_file,\n",
    "      #examples=example_gen.outputs['examples'],\n",
    "      examples=transform.outputs['transformed_examples'],\n",
    "      transform_graph=transform.outputs['transform_graph'],\n",
    "      schema=schema_gen.outputs['schema'],\n",
    "      train_args=tfx_v1.proto.TrainArgs(num_steps=100),\n",
    "      eval_args=tfx_v1.proto.EvalArgs(num_steps=5))\n",
    "\n",
    "  vertex_serving_spec = {\n",
    "      'project_id': project_id,\n",
    "      'endpoint_name': endpoint_name,\n",
    "      'machine_type': 'n1-standard-4',\n",
    "  }\n",
    "\n",
    "  serving_image = 'us-docker.pkg.dev/vertex-ai/prediction/tf2-cpu.2-6:latest'\n",
    "  pusher = tfx_v1.extensions.google_cloud_ai_platform.Pusher(\n",
    "      model=trainer.outputs['model'],\n",
    "      custom_config={\n",
    "          tfx_v1.extensions.google_cloud_ai_platform.ENABLE_VERTEX_KEY:\n",
    "              True,\n",
    "          tfx_v1.extensions.google_cloud_ai_platform.VERTEX_REGION_KEY:\n",
    "              region,\n",
    "          tfx_v1.extensions.google_cloud_ai_platform.VERTEX_CONTAINER_IMAGE_URI_KEY:\n",
    "              serving_image,\n",
    "          tfx_v1.extensions.google_cloud_ai_platform.SERVING_ARGS_KEY:\n",
    "            vertex_serving_spec,\n",
    "      })\n",
    "\n",
    "  components = [\n",
    "      example_gen,\n",
    "      compute_eval_stats,\n",
    "      schema_gen,\n",
    "      validate_stats,\n",
    "      transform,\n",
    "      trainer,\n",
    "      pusher,\n",
    "  ]\n",
    "\n",
    "  return tfx_v1.dsl.Pipeline(\n",
    "      pipeline_name=pipeline_name,\n",
    "      pipeline_root=pipeline_root,\n",
    "      components=components)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mJbq07THU2GV"
   },
   "source": [
    "### Running the pipeline on Vertex Pipelines.\n",
    "\n",
    "The following section compiles the pipeline using the Kubeflow V2 SDK and then starts a pipeline run on Vertex Pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {
    "id": "fAtfOZTYWJu-",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Excluding no splits because exclude_splits is not set.\n",
      "INFO:absl:Excluding no splits because exclude_splits is not set.\n",
      "INFO:absl:Excluding no splits because exclude_splits is not set.\n",
      "INFO:absl:Generating ephemeral wheel package for '/home/jupyter/new_dir/preprocess.py' (including modules: ['kubeflow_v2_runner', 'local_runner', 'preprocess', 'penguin_trainer', 'kubeflow_runner', 'taxi_trainer']).\n",
      "INFO:absl:User module package has hash fingerprint version f11ac71028092cf23345c26b66cebd05a08a0516d14f82a979e4f3c34a01fc2c.\n",
      "INFO:absl:Executing: ['/opt/conda/bin/python', '/tmp/tmpq0ah4pez/_tfx_generated_setup.py', 'bdist_wheel', '--bdist-dir', '/tmp/tmp9u6mpuxq', '--dist-dir', '/tmp/tmpjh5nf92s']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running bdist_wheel\n",
      "running build\n",
      "running build_py\n",
      "creating build\n",
      "creating build/lib\n",
      "copying kubeflow_v2_runner.py -> build/lib\n",
      "copying local_runner.py -> build/lib\n",
      "copying preprocess.py -> build/lib\n",
      "copying penguin_trainer.py -> build/lib\n",
      "copying kubeflow_runner.py -> build/lib\n",
      "copying taxi_trainer.py -> build/lib\n",
      "installing to /tmp/tmp9u6mpuxq\n",
      "running install\n",
      "running install_lib\n",
      "copying build/lib/taxi_trainer.py -> /tmp/tmp9u6mpuxq\n",
      "copying build/lib/penguin_trainer.py -> /tmp/tmp9u6mpuxq\n",
      "copying build/lib/kubeflow_v2_runner.py -> /tmp/tmp9u6mpuxq\n",
      "copying build/lib/local_runner.py -> /tmp/tmp9u6mpuxq\n",
      "copying build/lib/preprocess.py -> /tmp/tmp9u6mpuxq\n",
      "copying build/lib/kubeflow_runner.py -> /tmp/tmp9u6mpuxq\n",
      "running install_egg_info\n",
      "running egg_info\n",
      "creating tfx_user_code_Transform.egg-info\n",
      "writing tfx_user_code_Transform.egg-info/PKG-INFO\n",
      "writing dependency_links to tfx_user_code_Transform.egg-info/dependency_links.txt\n",
      "writing top-level names to tfx_user_code_Transform.egg-info/top_level.txt\n",
      "writing manifest file 'tfx_user_code_Transform.egg-info/SOURCES.txt'\n",
      "reading manifest file 'tfx_user_code_Transform.egg-info/SOURCES.txt'\n",
      "writing manifest file 'tfx_user_code_Transform.egg-info/SOURCES.txt'\n",
      "Copying tfx_user_code_Transform.egg-info to /tmp/tmp9u6mpuxq/tfx_user_code_Transform-0.0+f11ac71028092cf23345c26b66cebd05a08a0516d14f82a979e4f3c34a01fc2c-py3.7.egg-info\n",
      "running install_scripts\n",
      "creating /tmp/tmp9u6mpuxq/tfx_user_code_Transform-0.0+f11ac71028092cf23345c26b66cebd05a08a0516d14f82a979e4f3c34a01fc2c.dist-info/WHEEL\n",
      "creating '/tmp/tmpjh5nf92s/tfx_user_code_Transform-0.0+f11ac71028092cf23345c26b66cebd05a08a0516d14f82a979e4f3c34a01fc2c-py3-none-any.whl' and adding '/tmp/tmp9u6mpuxq' to it\n",
      "adding 'kubeflow_runner.py'\n",
      "adding 'kubeflow_v2_runner.py'\n",
      "adding 'local_runner.py'\n",
      "adding 'penguin_trainer.py'\n",
      "adding 'preprocess.py'\n",
      "adding 'taxi_trainer.py'\n",
      "adding 'tfx_user_code_Transform-0.0+f11ac71028092cf23345c26b66cebd05a08a0516d14f82a979e4f3c34a01fc2c.dist-info/METADATA'\n",
      "adding 'tfx_user_code_Transform-0.0+f11ac71028092cf23345c26b66cebd05a08a0516d14f82a979e4f3c34a01fc2c.dist-info/WHEEL'\n",
      "adding 'tfx_user_code_Transform-0.0+f11ac71028092cf23345c26b66cebd05a08a0516d14f82a979e4f3c34a01fc2c.dist-info/top_level.txt'\n",
      "adding 'tfx_user_code_Transform-0.0+f11ac71028092cf23345c26b66cebd05a08a0516d14f82a979e4f3c34a01fc2c.dist-info/RECORD'\n",
      "removing /tmp/tmp9u6mpuxq\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/setuptools/command/install.py:37: SetuptoolsDeprecationWarning: setup.py install is deprecated. Use build and pip and other standards-based tools.\n",
      "  setuptools.SetuptoolsDeprecationWarning,\n",
      "INFO:absl:Successfully built user code wheel distribution at 'gs://mormota/pipeline_root/taxi-vertex-pipelines/_wheels/tfx_user_code_Transform-0.0+f11ac71028092cf23345c26b66cebd05a08a0516d14f82a979e4f3c34a01fc2c-py3-none-any.whl'; target user module is 'preprocess'.\n",
      "INFO:absl:Full user module path is 'preprocess@gs://mormota/pipeline_root/taxi-vertex-pipelines/_wheels/tfx_user_code_Transform-0.0+f11ac71028092cf23345c26b66cebd05a08a0516d14f82a979e4f3c34a01fc2c-py3-none-any.whl'\n",
      "INFO:absl:Generating ephemeral wheel package for '/tmp/tmp92zglyww/taxi_trainer.py' (including modules: ['taxi_trainer']).\n",
      "INFO:absl:User module package has hash fingerprint version 86a8dae30f9470ac452bc47323cba22c89aa73987d7e6157d4dc3d89616a1103.\n",
      "INFO:absl:Executing: ['/opt/conda/bin/python', '/tmp/tmphwtxsu75/_tfx_generated_setup.py', 'bdist_wheel', '--bdist-dir', '/tmp/tmpb7i255j9', '--dist-dir', '/tmp/tmpw7s2m75z']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running bdist_wheel\n",
      "running build\n",
      "running build_py\n",
      "creating build\n",
      "creating build/lib\n",
      "copying taxi_trainer.py -> build/lib\n",
      "installing to /tmp/tmpb7i255j9\n",
      "running install\n",
      "running install_lib\n",
      "copying build/lib/taxi_trainer.py -> /tmp/tmpb7i255j9\n",
      "running install_egg_info\n",
      "running egg_info\n",
      "creating tfx_user_code_Trainer.egg-info\n",
      "writing tfx_user_code_Trainer.egg-info/PKG-INFO\n",
      "writing dependency_links to tfx_user_code_Trainer.egg-info/dependency_links.txt\n",
      "writing top-level names to tfx_user_code_Trainer.egg-info/top_level.txt\n",
      "writing manifest file 'tfx_user_code_Trainer.egg-info/SOURCES.txt'\n",
      "reading manifest file 'tfx_user_code_Trainer.egg-info/SOURCES.txt'\n",
      "writing manifest file 'tfx_user_code_Trainer.egg-info/SOURCES.txt'\n",
      "Copying tfx_user_code_Trainer.egg-info to /tmp/tmpb7i255j9/tfx_user_code_Trainer-0.0+86a8dae30f9470ac452bc47323cba22c89aa73987d7e6157d4dc3d89616a1103-py3.7.egg-info\n",
      "running install_scripts\n",
      "creating /tmp/tmpb7i255j9/tfx_user_code_Trainer-0.0+86a8dae30f9470ac452bc47323cba22c89aa73987d7e6157d4dc3d89616a1103.dist-info/WHEEL\n",
      "creating '/tmp/tmpw7s2m75z/tfx_user_code_Trainer-0.0+86a8dae30f9470ac452bc47323cba22c89aa73987d7e6157d4dc3d89616a1103-py3-none-any.whl' and adding '/tmp/tmpb7i255j9' to it\n",
      "adding 'taxi_trainer.py'\n",
      "adding 'tfx_user_code_Trainer-0.0+86a8dae30f9470ac452bc47323cba22c89aa73987d7e6157d4dc3d89616a1103.dist-info/METADATA'\n",
      "adding 'tfx_user_code_Trainer-0.0+86a8dae30f9470ac452bc47323cba22c89aa73987d7e6157d4dc3d89616a1103.dist-info/WHEEL'\n",
      "adding 'tfx_user_code_Trainer-0.0+86a8dae30f9470ac452bc47323cba22c89aa73987d7e6157d4dc3d89616a1103.dist-info/top_level.txt'\n",
      "adding 'tfx_user_code_Trainer-0.0+86a8dae30f9470ac452bc47323cba22c89aa73987d7e6157d4dc3d89616a1103.dist-info/RECORD'\n",
      "removing /tmp/tmpb7i255j9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/setuptools/command/install.py:37: SetuptoolsDeprecationWarning: setup.py install is deprecated. Use build and pip and other standards-based tools.\n",
      "  setuptools.SetuptoolsDeprecationWarning,\n",
      "INFO:absl:Successfully built user code wheel distribution at 'gs://mormota/pipeline_root/taxi-vertex-pipelines/_wheels/tfx_user_code_Trainer-0.0+86a8dae30f9470ac452bc47323cba22c89aa73987d7e6157d4dc3d89616a1103-py3-none-any.whl'; target user module is 'taxi_trainer'.\n",
      "INFO:absl:Full user module path is 'taxi_trainer@gs://mormota/pipeline_root/taxi-vertex-pipelines/_wheels/tfx_user_code_Trainer-0.0+86a8dae30f9470ac452bc47323cba22c89aa73987d7e6157d4dc3d89616a1103-py3-none-any.whl'\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "PIPELINE_DEFINITION_FILE = PIPELINE_NAME + '_pipeline.json'\n",
    "\n",
    "runner = tfx_v1.orchestration.experimental.KubeflowV2DagRunner(\n",
    "    config=tfx_v1.orchestration.experimental.KubeflowV2DagRunnerConfig(),\n",
    "    output_filename=PIPELINE_DEFINITION_FILE)\n",
    "_ = runner.run(\n",
    "    _create_pipeline(\n",
    "        pipeline_name=PIPELINE_NAME,\n",
    "        pipeline_root=PIPELINE_ROOT,\n",
    "        data_root=DATA_ROOT,\n",
    "        module_file=os.path.join(MODULE_ROOT, _trainer_module_file),\n",
    "        endpoint_name=ENDPOINT_NAME,\n",
    "        project_id=GOOGLE_CLOUD_PROJECT,\n",
    "        region=GOOGLE_CLOUD_REGION,\n",
    "        serving_model_dir=SERVING_MODEL_DIR))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {
    "id": "tI71jlEvWMV7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating PipelineJob\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.pipeline_jobs:Creating PipelineJob\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PipelineJob created. Resource name: projects/639006805448/locations/us-central1/pipelineJobs/taxi-vertex-pipelines-20220608053153\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob created. Resource name: projects/639006805448/locations/us-central1/pipelineJobs/taxi-vertex-pipelines-20220608053153\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To use this PipelineJob in another session:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.pipeline_jobs:To use this PipelineJob in another session:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pipeline_job = aiplatform.PipelineJob.get('projects/639006805448/locations/us-central1/pipelineJobs/taxi-vertex-pipelines-20220608053153')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.pipeline_jobs:pipeline_job = aiplatform.PipelineJob.get('projects/639006805448/locations/us-central1/pipelineJobs/taxi-vertex-pipelines-20220608053153')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View Pipeline Job:\n",
      "https://console.cloud.google.com/vertex-ai/locations/us-central1/pipelines/runs/taxi-vertex-pipelines-20220608053153?project=639006805448\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.pipeline_jobs:View Pipeline Job:\n",
      "https://console.cloud.google.com/vertex-ai/locations/us-central1/pipelines/runs/taxi-vertex-pipelines-20220608053153?project=639006805448\n"
     ]
    }
   ],
   "source": [
    "from google.cloud import aiplatform\n",
    "from google.cloud.aiplatform import pipeline_jobs\n",
    "import logging\n",
    "logging.getLogger().setLevel(logging.INFO)\n",
    "\n",
    "aiplatform.init(project=GOOGLE_CLOUD_PROJECT, location=GOOGLE_CLOUD_REGION)\n",
    "\n",
    "job = pipeline_jobs.PipelineJob(template_path=PIPELINE_DEFINITION_FILE,\n",
    "                                display_name=PIPELINE_NAME)\n",
    "job.submit()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "pknVo1kM2wI2"
   ],
   "name": "Simple TFX Pipeline for Vertex Pipelines",
   "provenance": [],
   "toc_visible": true
  },
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-8.m93",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-8:m93"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
