{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline definition and running\n",
    "\n",
    "This notebook serves as the core of the assignment solution for the Chicago Taxi Trips task. It defines all the necessary steps and runs the pipeline on Vertex Pipelines. The other notebooks will refer back to parts of this notebook or use the results of a pipeline run created here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "Xd-iP9wEaENu",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.8.1\n",
      "TFX version: 1.8.0\n",
      "KFP version: 1.8.12\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print('TensorFlow version: {}'.format(tf.__version__))\n",
    "from tfx import v1 as tfx_v1\n",
    "import tfx\n",
    "print('TFX version: {}'.format(tfx.__version__))\n",
    "import kfp\n",
    "print('KFP version: {}'.format(kfp.__version__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data.queries import TRAIN_QUERY, EVAL_QUERY, TEST_QUERY\n",
    "from pipeline.pipeline_def import _create_pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aDtLdSkvqPHe"
   },
   "source": [
    "### Setting up variables\n",
    "\n",
    "This section sets up the GCP variables used for the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "EcUseqJaE2XN",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PIPELINE_ROOT: gs://aliz-ml-spec-2022/demo-1/pipeline_root/taxi-vertex-pipelines\n"
     ]
    }
   ],
   "source": [
    "GOOGLE_CLOUD_PROJECT = 'aliz-ml-spec-2022-submission'\n",
    "GOOGLE_CLOUD_REGION = 'us-central1'\n",
    "GCS_BUCKET_NAME = 'aliz-ml-spec-2022'\n",
    "\n",
    "PIPELINE_NAME = 'taxi-vertex-pipelines'\n",
    "\n",
    "# Path to various pipeline artifact.\n",
    "PIPELINE_ROOT = 'gs://{}/demo-1/pipeline_root/{}'.format(\n",
    "    GCS_BUCKET_NAME, PIPELINE_NAME)\n",
    "\n",
    "# Paths for users' Python module.\n",
    "MODULE_ROOT = 'gs://{}/demo-1/pipeline_module/{}'.format(\n",
    "    GCS_BUCKET_NAME, PIPELINE_NAME)\n",
    "\n",
    "# Paths for input data.\n",
    "DATA_ROOT = 'gs://{}/demo-1/data/taxi_data'.format(GCS_BUCKET_NAME)\n",
    "\n",
    "# This is the path where your model will be pushed for serving.\n",
    "SERVING_MODEL_DIR = 'gs://{}/demo-1/serving_model/{}'.format(\n",
    "    GCS_BUCKET_NAME, PIPELINE_NAME)\n",
    "\n",
    "ENDPOINT_NAME = 'prediction-' + PIPELINE_NAME\n",
    "\n",
    "print('PIPELINE_ROOT: {}'.format(PIPELINE_ROOT))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8F2SRwRLSYGa"
   },
   "source": [
    "### Prepare data\n",
    "\n",
    "To simplify the process, we are going to use the CsvExampleGen component. To do so, we export the required portion of the dataset to a GCS bucket using the query below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Query complete after 0.01s: 100%|██████████| 1/1 [00:00<00:00, 408.48query/s]                          \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: []"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%bigquery\n",
    "\n",
    "$TRAIN_QUERY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Query complete after 0.00s: 100%|██████████| 1/1 [00:00<00:00, 380.09query/s]                          \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: []"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%bigquery\n",
    "\n",
    "$EVAL_QUERY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Query complete after 0.00s: 100%|██████████| 1/1 [00:00<00:00, 518.14query/s]                          \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: []"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%bigquery\n",
    "\n",
    "$TEST_QUERY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nH6gizcpSwWV"
   },
   "source": [
    "### Pipeline modules\n",
    "\n",
    "The following blocks create the preprocessing and model module code and upload the files to GCS. This last step is necessary for TFX to be able to use our custom code in the different components.\n",
    "\n",
    "#### Preprocessing\n",
    "\n",
    "The preprocessing transforms the categorical features into vocabularies and standardises the numerical features.\n",
    "\n",
    "#### Model\n",
    "\n",
    "The model is a simple DNN that is made up of two blocks. The first processes the inputs and embeds taxi trips in an implicit feature space. The second is a dense block that leads to the final predictions.\n",
    "\n",
    "#### Hyperparameter tuning\n",
    "\n",
    "The hyperparameter tuning section uses Keras tuning to perform a hyperparameter search on the previously defined model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "_trainer_module_file_location = '../src/pipeline/taxi_trainer.py'\n",
    "preprocess_module_file_location = '../src/pipeline/preprocess.py'\n",
    "_trainer_module_file = 'taxi_trainer.py'\n",
    "preprocess_module_file = 'preprocess.py'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-LsYx8MpYvPv"
   },
   "source": [
    "Copy the module file to GCS which can be accessed from the pipeline components.\n",
    "Because model training happens on GCP, we need to upload this model definition. \n",
    "\n",
    "Otherwise, you might want to build a container image including the module file\n",
    "and use the image to run the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "rMMs5wuNYAbc",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying file://../src/pipeline/taxi_trainer.py [Content-Type=text/x-python]...\n",
      "- [1 files][  5.2 KiB/  5.2 KiB]                                                \n",
      "Operation completed over 1 objects/5.2 KiB.                                      \n"
     ]
    }
   ],
   "source": [
    "!gsutil cp {_trainer_module_file_location} {MODULE_ROOT}/taxi_trainer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying file://../src/pipeline/preprocess.py [Content-Type=text/x-python]...\n",
      "- [1 files][  2.2 KiB/  2.2 KiB]                                                \n",
      "Operation completed over 1 objects/2.2 KiB.                                      \n"
     ]
    }
   ],
   "source": [
    "!gsutil cp {preprocess_module_file_location} {MODULE_ROOT}/preprocess.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w3OkNz3gTLwM"
   },
   "source": [
    "### Pipeline definition\n",
    "\n",
    "The following block replicates the pipeline creation found in src/pipeline/pipeline_def to showcase the creation of a TFX Pipeline. It is made up of the following components:\n",
    "\n",
    "- CsvExampleGen: Loads our dataset from Google Cloud Storage\n",
    "- SchemaGen: Creates a TF schema from our dataset to be used in later components\n",
    "- ExampleValidator: Allows us to check for data skew later on\n",
    "- Transform: Performs preprocessing on the data\n",
    "- Tuner: Performs hyperparameter tuning\n",
    "- Trainer: Trains the model\n",
    "- Pusher: Deploys the model to the endpoint on Vertex AI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "M49yYVNBTPd4",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import tensorflow_model_analysis as tfma\n",
    "\n",
    "def _create_pipeline(pipeline_name: str, pipeline_root: str, data_root: str,\n",
    "                     module_file: str, serving_model_dir: str, project_id: str,\n",
    "                     endpoint_name: str, region: str,\n",
    "                     ) -> tfx_v1.dsl.Pipeline:\n",
    "  \"\"\"Defines a pipeline for the Chicago Taxi Trips assignment\"\"\"\n",
    "\n",
    "  input = tfx_v1.proto.Input(splits=[\n",
    "                tfx.proto.example_gen_pb2.Input.Split(name='train', pattern='train/*'),\n",
    "                tfx.proto.example_gen_pb2.Input.Split(name='eval', pattern='eval/*'),\n",
    "                tfx.proto.example_gen_pb2.Input.Split(name='test', pattern='test/*')\n",
    "            ])\n",
    "  example_gen = tfx_v1.components.CsvExampleGen(input_base=data_root, input_config=input)\n",
    "\n",
    "  compute_eval_stats = tfx_v1.components.StatisticsGen(\n",
    "      examples=example_gen.outputs['examples'],\n",
    "      )\n",
    "  schema_gen = tfx_v1.components.SchemaGen(\n",
    "    statistics=compute_eval_stats.outputs['statistics'])\n",
    "    \n",
    "  validate_stats = tfx_v1.components.ExampleValidator(\n",
    "      statistics=compute_eval_stats.outputs['statistics'],\n",
    "      schema=schema_gen.outputs['schema']\n",
    "      )\n",
    "\n",
    "  transform = tfx_v1.components.Transform(\n",
    "    examples=example_gen.outputs['examples'],\n",
    "    schema=schema_gen.outputs['schema'],\n",
    "    module_file=preprocess_module_file_location)\n",
    "    \n",
    "  vertex_job_spec = {\n",
    "      'project': project_id,\n",
    "      'worker_pool_specs': [{\n",
    "          'machine_spec': {\n",
    "              'machine_type': 'n1-standard-4',\n",
    "          },\n",
    "          'replica_count': 1,\n",
    "          'container_spec': {\n",
    "              'image_uri': 'gcr.io/tfx-oss-public/tfx:{}'.format(tfx.__version__),\n",
    "          },\n",
    "      }],\n",
    "  }\n",
    "\n",
    "  tuner = tfx_v1.components.Tuner(\n",
    "    module_file=module_file,\n",
    "    examples=transform.outputs['transformed_examples'],\n",
    "    transform_graph=transform.outputs['transform_graph'],\n",
    "    train_args=tfx_v1.proto.TrainArgs(num_steps=20),\n",
    "    schema=schema_gen.outputs['schema'],\n",
    "    eval_args=tfx_v1.proto.EvalArgs(num_steps=5))\n",
    "    \n",
    "    \n",
    "  trainer = tfx.v1.extensions.google_cloud_ai_platform.Trainer(\n",
    "      module_file=module_file,\n",
    "      examples=transform.outputs['transformed_examples'],\n",
    "      transform_graph=transform.outputs['transform_graph'],\n",
    "      schema=schema_gen.outputs['schema'],\n",
    "      hyperparameters=tuner.outputs['best_hyperparameters'],\n",
    "      train_args=tfx_v1.proto.TrainArgs(num_steps=100),\n",
    "      eval_args=tfx_v1.proto.EvalArgs(num_steps=5),\n",
    "      custom_config={\n",
    "          tfx_v1.extensions.google_cloud_ai_platform.ENABLE_VERTEX_KEY:\n",
    "              True,\n",
    "          tfx_v1.extensions.google_cloud_ai_platform.VERTEX_REGION_KEY:\n",
    "              region,\n",
    "          tfx_v1.extensions.google_cloud_ai_platform.TRAINING_ARGS_KEY:\n",
    "              vertex_job_spec,\n",
    "          'use_gpu':\n",
    "              False,\n",
    "      })\n",
    "\n",
    "\n",
    "  eval_config = tfma.EvalConfig(\n",
    "    model_specs=[\n",
    "        tfma.ModelSpec(label_key='fare')\n",
    "    ],\n",
    "    metrics_specs=[\n",
    "        tfma.MetricsSpec(\n",
    "            metrics=[\n",
    "                tfma.MetricConfig(class_name='ExampleCount'),\n",
    "                tfma.MetricConfig(class_name='MeanSquaredError'),\n",
    "                tfma.MetricConfig(\n",
    "                    class_name='MeanSquaredError',\n",
    "                    threshold=tfma.MetricThreshold(\n",
    "                        value_threshold=tfma.GenericValueThreshold(\n",
    "                            upper_bound={'value': 50}),))\n",
    "            ]\n",
    "        )\n",
    "    ],\n",
    "    slicing_specs=[\n",
    "        tfma.SlicingSpec(),\n",
    "        tfma.SlicingSpec(feature_keys=['TripStartHour'])\n",
    "    ])\n",
    "\n",
    "\n",
    "\n",
    "  model_analyzer = tfx_v1.components.Evaluator(\n",
    "      examples=transform.outputs['transformed_examples'],\n",
    "      model=trainer.outputs['model'],\n",
    "      eval_config=eval_config)\n",
    "\n",
    "  vertex_serving_spec = {\n",
    "      'project_id': project_id,\n",
    "      'endpoint_name': endpoint_name,\n",
    "      'machine_type': 'n1-standard-4',\n",
    "  }\n",
    "\n",
    "  serving_image = 'us-docker.pkg.dev/vertex-ai/prediction/tf2-cpu.2-6:latest'\n",
    "  pusher = tfx_v1.extensions.google_cloud_ai_platform.Pusher(\n",
    "      model=trainer.outputs['model'],\n",
    "      custom_config={\n",
    "          tfx_v1.extensions.google_cloud_ai_platform.ENABLE_VERTEX_KEY:\n",
    "              True,\n",
    "          tfx_v1.extensions.google_cloud_ai_platform.VERTEX_REGION_KEY:\n",
    "              region,\n",
    "          tfx_v1.extensions.google_cloud_ai_platform.VERTEX_CONTAINER_IMAGE_URI_KEY:\n",
    "              serving_image,\n",
    "          tfx_v1.extensions.google_cloud_ai_platform.SERVING_ARGS_KEY:\n",
    "            vertex_serving_spec,\n",
    "      })\n",
    "\n",
    "  components = [\n",
    "      example_gen,\n",
    "      compute_eval_stats,\n",
    "      schema_gen,\n",
    "      validate_stats,\n",
    "      transform,\n",
    "      tuner,\n",
    "      trainer,\n",
    "      model_analyzer,\n",
    "      pusher,\n",
    "  ]\n",
    "\n",
    "  return tfx_v1.dsl.Pipeline(\n",
    "      pipeline_name=pipeline_name,\n",
    "      pipeline_root=pipeline_root,\n",
    "      components=components,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mJbq07THU2GV"
   },
   "source": [
    "### Running the pipeline on Vertex Pipelines.\n",
    "\n",
    "The following section compiles the pipeline using the Kubeflow V2 SDK and then starts a pipeline run on Vertex Pipelines. Note that no pipeline run is created at this point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "fAtfOZTYWJu-",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running bdist_wheel\n",
      "running build\n",
      "running build_py\n",
      "creating build\n",
      "creating build/lib\n",
      "copying pipeline_def.py -> build/lib\n",
      "copying taxi_trainer.py -> build/lib\n",
      "copying preprocess.py -> build/lib\n",
      "installing to /tmp/tmpflm5rlcs\n",
      "running install\n",
      "running install_lib\n",
      "copying build/lib/taxi_trainer.py -> /tmp/tmpflm5rlcs\n",
      "copying build/lib/pipeline_def.py -> /tmp/tmpflm5rlcs\n",
      "copying build/lib/preprocess.py -> /tmp/tmpflm5rlcs\n",
      "running install_egg_info\n",
      "running egg_info\n",
      "creating tfx_user_code_Transform.egg-info\n",
      "writing tfx_user_code_Transform.egg-info/PKG-INFO\n",
      "writing dependency_links to tfx_user_code_Transform.egg-info/dependency_links.txt\n",
      "writing top-level names to tfx_user_code_Transform.egg-info/top_level.txt\n",
      "writing manifest file 'tfx_user_code_Transform.egg-info/SOURCES.txt'\n",
      "reading manifest file 'tfx_user_code_Transform.egg-info/SOURCES.txt'\n",
      "writing manifest file 'tfx_user_code_Transform.egg-info/SOURCES.txt'\n",
      "Copying tfx_user_code_Transform.egg-info to /tmp/tmpflm5rlcs/tfx_user_code_Transform-0.0+ae99c8570544eef4a8e89bacf26be967e0a58b8bd0ca5627edcde7416690cb0a-py3.7.egg-info\n",
      "running install_scripts\n",
      "creating /tmp/tmpflm5rlcs/tfx_user_code_Transform-0.0+ae99c8570544eef4a8e89bacf26be967e0a58b8bd0ca5627edcde7416690cb0a.dist-info/WHEEL\n",
      "creating '/tmp/tmpwve8jal5/tfx_user_code_Transform-0.0+ae99c8570544eef4a8e89bacf26be967e0a58b8bd0ca5627edcde7416690cb0a-py3-none-any.whl' and adding '/tmp/tmpflm5rlcs' to it\n",
      "adding 'pipeline_def.py'\n",
      "adding 'preprocess.py'\n",
      "adding 'taxi_trainer.py'\n",
      "adding 'tfx_user_code_Transform-0.0+ae99c8570544eef4a8e89bacf26be967e0a58b8bd0ca5627edcde7416690cb0a.dist-info/METADATA'\n",
      "adding 'tfx_user_code_Transform-0.0+ae99c8570544eef4a8e89bacf26be967e0a58b8bd0ca5627edcde7416690cb0a.dist-info/WHEEL'\n",
      "adding 'tfx_user_code_Transform-0.0+ae99c8570544eef4a8e89bacf26be967e0a58b8bd0ca5627edcde7416690cb0a.dist-info/top_level.txt'\n",
      "adding 'tfx_user_code_Transform-0.0+ae99c8570544eef4a8e89bacf26be967e0a58b8bd0ca5627edcde7416690cb0a.dist-info/RECORD'\n",
      "removing /tmp/tmpflm5rlcs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/setuptools/command/install.py:37: SetuptoolsDeprecationWarning: setup.py install is deprecated. Use build and pip and other standards-based tools.\n",
      "  setuptools.SetuptoolsDeprecationWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running bdist_wheel\n",
      "running build\n",
      "running build_py\n",
      "creating build\n",
      "creating build/lib\n",
      "copying taxi_trainer.py -> build/lib\n",
      "installing to /tmp/tmpzn0jysg8\n",
      "running install\n",
      "running install_lib\n",
      "copying build/lib/taxi_trainer.py -> /tmp/tmpzn0jysg8\n",
      "running install_egg_info\n",
      "running egg_info\n",
      "creating tfx_user_code_Tuner.egg-info\n",
      "writing tfx_user_code_Tuner.egg-info/PKG-INFO\n",
      "writing dependency_links to tfx_user_code_Tuner.egg-info/dependency_links.txt\n",
      "writing top-level names to tfx_user_code_Tuner.egg-info/top_level.txt\n",
      "writing manifest file 'tfx_user_code_Tuner.egg-info/SOURCES.txt'\n",
      "reading manifest file 'tfx_user_code_Tuner.egg-info/SOURCES.txt'\n",
      "writing manifest file 'tfx_user_code_Tuner.egg-info/SOURCES.txt'\n",
      "Copying tfx_user_code_Tuner.egg-info to /tmp/tmpzn0jysg8/tfx_user_code_Tuner-0.0+1e86b02980eae8be19a8e9caef2ee6f049dfd20fb4ac9c12d379fc635601c8d0-py3.7.egg-info\n",
      "running install_scripts\n",
      "creating /tmp/tmpzn0jysg8/tfx_user_code_Tuner-0.0+1e86b02980eae8be19a8e9caef2ee6f049dfd20fb4ac9c12d379fc635601c8d0.dist-info/WHEEL\n",
      "creating '/tmp/tmpgvejx4xw/tfx_user_code_Tuner-0.0+1e86b02980eae8be19a8e9caef2ee6f049dfd20fb4ac9c12d379fc635601c8d0-py3-none-any.whl' and adding '/tmp/tmpzn0jysg8' to it\n",
      "adding 'taxi_trainer.py'\n",
      "adding 'tfx_user_code_Tuner-0.0+1e86b02980eae8be19a8e9caef2ee6f049dfd20fb4ac9c12d379fc635601c8d0.dist-info/METADATA'\n",
      "adding 'tfx_user_code_Tuner-0.0+1e86b02980eae8be19a8e9caef2ee6f049dfd20fb4ac9c12d379fc635601c8d0.dist-info/WHEEL'\n",
      "adding 'tfx_user_code_Tuner-0.0+1e86b02980eae8be19a8e9caef2ee6f049dfd20fb4ac9c12d379fc635601c8d0.dist-info/top_level.txt'\n",
      "adding 'tfx_user_code_Tuner-0.0+1e86b02980eae8be19a8e9caef2ee6f049dfd20fb4ac9c12d379fc635601c8d0.dist-info/RECORD'\n",
      "removing /tmp/tmpzn0jysg8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/setuptools/command/install.py:37: SetuptoolsDeprecationWarning: setup.py install is deprecated. Use build and pip and other standards-based tools.\n",
      "  setuptools.SetuptoolsDeprecationWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running bdist_wheel\n",
      "running build\n",
      "running build_py\n",
      "creating build\n",
      "creating build/lib\n",
      "copying taxi_trainer.py -> build/lib\n",
      "installing to /tmp/tmpwvqsrehe\n",
      "running install\n",
      "running install_lib\n",
      "copying build/lib/taxi_trainer.py -> /tmp/tmpwvqsrehe\n",
      "running install_egg_info\n",
      "running egg_info\n",
      "creating tfx_user_code_Trainer.egg-info\n",
      "writing tfx_user_code_Trainer.egg-info/PKG-INFO\n",
      "writing dependency_links to tfx_user_code_Trainer.egg-info/dependency_links.txt\n",
      "writing top-level names to tfx_user_code_Trainer.egg-info/top_level.txt\n",
      "writing manifest file 'tfx_user_code_Trainer.egg-info/SOURCES.txt'\n",
      "reading manifest file 'tfx_user_code_Trainer.egg-info/SOURCES.txt'\n",
      "writing manifest file 'tfx_user_code_Trainer.egg-info/SOURCES.txt'\n",
      "Copying tfx_user_code_Trainer.egg-info to /tmp/tmpwvqsrehe/tfx_user_code_Trainer-0.0+1e86b02980eae8be19a8e9caef2ee6f049dfd20fb4ac9c12d379fc635601c8d0-py3.7.egg-info\n",
      "running install_scripts\n",
      "creating /tmp/tmpwvqsrehe/tfx_user_code_Trainer-0.0+1e86b02980eae8be19a8e9caef2ee6f049dfd20fb4ac9c12d379fc635601c8d0.dist-info/WHEEL\n",
      "creating '/tmp/tmpoaloh3mq/tfx_user_code_Trainer-0.0+1e86b02980eae8be19a8e9caef2ee6f049dfd20fb4ac9c12d379fc635601c8d0-py3-none-any.whl' and adding '/tmp/tmpwvqsrehe' to it\n",
      "adding 'taxi_trainer.py'\n",
      "adding 'tfx_user_code_Trainer-0.0+1e86b02980eae8be19a8e9caef2ee6f049dfd20fb4ac9c12d379fc635601c8d0.dist-info/METADATA'\n",
      "adding 'tfx_user_code_Trainer-0.0+1e86b02980eae8be19a8e9caef2ee6f049dfd20fb4ac9c12d379fc635601c8d0.dist-info/WHEEL'\n",
      "adding 'tfx_user_code_Trainer-0.0+1e86b02980eae8be19a8e9caef2ee6f049dfd20fb4ac9c12d379fc635601c8d0.dist-info/top_level.txt'\n",
      "adding 'tfx_user_code_Trainer-0.0+1e86b02980eae8be19a8e9caef2ee6f049dfd20fb4ac9c12d379fc635601c8d0.dist-info/RECORD'\n",
      "removing /tmp/tmpwvqsrehe\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/setuptools/command/install.py:37: SetuptoolsDeprecationWarning: setup.py install is deprecated. Use build and pip and other standards-based tools.\n",
      "  setuptools.SetuptoolsDeprecationWarning,\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "PIPELINE_DEFINITION_FILE = PIPELINE_NAME + '_pipeline.json'\n",
    "\n",
    "runner = tfx_v1.orchestration.experimental.KubeflowV2DagRunner(\n",
    "    config=tfx_v1.orchestration.experimental.KubeflowV2DagRunnerConfig(),\n",
    "    output_filename=PIPELINE_DEFINITION_FILE)\n",
    "_ = runner.run(\n",
    "    _create_pipeline(\n",
    "        pipeline_name=PIPELINE_NAME,\n",
    "        pipeline_root=PIPELINE_ROOT,\n",
    "        data_root=DATA_ROOT,\n",
    "        module_file=os.path.join(MODULE_ROOT, _trainer_module_file),\n",
    "        endpoint_name=ENDPOINT_NAME,\n",
    "        project_id=GOOGLE_CLOUD_PROJECT,\n",
    "        region=GOOGLE_CLOUD_REGION,\n",
    "        serving_model_dir=SERVING_MODEL_DIR,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "tI71jlEvWMV7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating PipelineJob\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.pipeline_jobs:Creating PipelineJob\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PipelineJob created. Resource name: projects/53911330556/locations/us-central1/pipelineJobs/taxi-vertex-pipelines-20220621095033\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob created. Resource name: projects/53911330556/locations/us-central1/pipelineJobs/taxi-vertex-pipelines-20220621095033\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To use this PipelineJob in another session:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.pipeline_jobs:To use this PipelineJob in another session:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pipeline_job = aiplatform.PipelineJob.get('projects/53911330556/locations/us-central1/pipelineJobs/taxi-vertex-pipelines-20220621095033')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.pipeline_jobs:pipeline_job = aiplatform.PipelineJob.get('projects/53911330556/locations/us-central1/pipelineJobs/taxi-vertex-pipelines-20220621095033')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View Pipeline Job:\n",
      "https://console.cloud.google.com/vertex-ai/locations/us-central1/pipelines/runs/taxi-vertex-pipelines-20220621095033?project=53911330556\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.pipeline_jobs:View Pipeline Job:\n",
      "https://console.cloud.google.com/vertex-ai/locations/us-central1/pipelines/runs/taxi-vertex-pipelines-20220621095033?project=53911330556\n"
     ]
    }
   ],
   "source": [
    "from google.cloud import aiplatform\n",
    "from google.cloud.aiplatform import pipeline_jobs\n",
    "import logging\n",
    "logging.getLogger().setLevel(logging.INFO)\n",
    "\n",
    "aiplatform.init(project=GOOGLE_CLOUD_PROJECT, location=GOOGLE_CLOUD_REGION)\n",
    "\n",
    "job = pipeline_jobs.PipelineJob(template_path=PIPELINE_DEFINITION_FILE,\n",
    "                                display_name=PIPELINE_NAME)\n",
    "job.submit()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "pknVo1kM2wI2"
   ],
   "name": "Simple TFX Pipeline for Vertex Pipelines",
   "provenance": [],
   "toc_visible": true
  },
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-8.m93",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-8:m93"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
